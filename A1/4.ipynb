{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**\n",
        "\n",
        "[Google Colab](https://colab.research.google.com/drive/18UVsYGcFUgsWPmJYAd8jb6iglFKcYTqx?usp=sharing)"
      ],
      "metadata": {
        "id": "NMJO6Qsqr4sV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoAZppGpnAEn",
        "outputId": "58eea9ec-300d-4587-cc04-33a411d6723a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Informer2020'...\n",
            "remote: Enumerating objects: 576, done.\u001b[K\n",
            "remote: Counting objects: 100% (222/222), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 576 (delta 199), reused 188 (delta 188), pack-reused 354\u001b[K\n",
            "Receiving objects: 100% (576/576), 6.48 MiB | 10.86 MiB/s, done.\n",
            "Resolving deltas: 100% (336/336), done.\n",
            "Cloning into 'ETDataset'...\n",
            "remote: Enumerating objects: 187, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 187 (delta 25), reused 20 (delta 20), pack-reused 159\u001b[K\n",
            "Receiving objects: 100% (187/187), 3.86 MiB | 7.19 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone necessary repositories\n",
        "!git clone https://github.com/zhouhaoyi/Informer2020.git\n",
        "!git clone https://github.com/zhouhaoyi/ETDataset.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if not 'Informer2020' in sys.path:\n",
        "    sys.path += ['Informer2020']"
      ],
      "metadata": {
        "id": "63rgheG8nXJw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules and libraries\n",
        "from utils.tools import dotdict\n",
        "from exp.exp_informer import Exp_Informer\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ],
      "metadata": {
        "id": "r0Tn4n8QnXug"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define configuration settings\n",
        "args = dotdict()\n",
        "args.model = 'informer'\n",
        "args.data = 'ETTh1'\n",
        "args.root_path = './ETDataset/ETT-small/'\n",
        "args.data_path = 'ETTh1.csv'\n",
        "args.features = 'M'\n",
        "args.target = 'OT'\n",
        "args.freq = 'h'\n",
        "args.checkpoints = './informer_checkpoints'\n",
        "\n",
        "args.seq_len = 96\n",
        "args.label_len = 48\n",
        "args.pred_len = 24\n",
        "\n",
        "args.enc_in = 7\n",
        "args.dec_in = 7\n",
        "args.c_out = 7\n",
        "args.factor = 5\n",
        "args.d_model = 512\n",
        "args.n_heads = 8\n",
        "args.e_layers = 2\n",
        "args.d_layers = 1\n",
        "args.d_ff = 2048\n",
        "args.dropout = 0.05\n",
        "args.embed = 'timeF'\n",
        "args.activation = 'gelu'\n",
        "args.distil = True\n",
        "args.output_attention = False\n",
        "args.mix = True\n",
        "args.padding = 0\n",
        "args.freq = 'h'\n",
        "\n",
        "args.batch_size = 32\n",
        "args.learning_rate = 0.0001\n",
        "args.loss = 'mse'\n",
        "args.lradj = 'type1'\n",
        "args.use_amp = False\n",
        "\n",
        "args.num_workers = 0\n",
        "args.itr = 1\n",
        "args.train_epochs = 6\n",
        "args.patience = 3\n",
        "args.des = 'exp'\n",
        "\n",
        "args.use_gpu = True if torch.cuda.is_available() else False\n",
        "args.gpu = 0\n",
        "\n",
        "args.use_multi_gpu = False\n",
        "args.devices = '0,1,2,3'\n"
      ],
      "metadata": {
        "id": "IaI1I7eInZq3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the attention type to 'prob' for Probsparse attention\n",
        "args.attn = 'prob'\n",
        "\n",
        "# Define a function to train and test the model\n",
        "def train_and_test(model_type):\n",
        "    setting = f'{model_type}'\n",
        "\n",
        "    exp = Exp_Informer(args)\n",
        "\n",
        "    print(f'---------------------- Start training: {setting} ---------------------->')\n",
        "    exp.train(setting)\n",
        "\n",
        "    print(f'---------------------- Testing: {setting} ----------------------')\n",
        "    exp.test(setting)\n",
        "\n",
        "# Train and test the Probsparse attention model\n",
        "train_and_test('Probsparse')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN9xRpzEoqTg",
        "outputId": "edde3adc-0d96-4552-ae45-5c0ece9396a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: cuda:0\n",
            "---------------------- Start training: Probsparse ---------------------->\n",
            "train 8521\n",
            "val 2857\n",
            "test 2857\n",
            "\titers: 100, epoch: 1 | loss: 0.4764659\n",
            "\tspeed: 0.1481s/iter; left time: 221.7066s\n",
            "\titers: 200, epoch: 1 | loss: 0.3557161\n",
            "\tspeed: 0.0683s/iter; left time: 95.3603s\n",
            "Epoch: 1 cost time: 25.803321599960327\n",
            "Epoch: 1, Steps: 266 | Train Loss: 0.4169796 Vali Loss: 0.6821633 Test Loss: 0.5922198\n",
            "Validation loss decreased (inf --> 0.682163).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.2703295\n",
            "\tspeed: 0.1625s/iter; left time: 199.9824s\n",
            "\titers: 200, epoch: 2 | loss: 0.1978941\n",
            "\tspeed: 0.0701s/iter; left time: 79.2286s\n",
            "Epoch: 2 cost time: 18.69873809814453\n",
            "Epoch: 2, Steps: 266 | Train Loss: 0.2502195 Vali Loss: 0.6543479 Test Loss: 0.5604963\n",
            "Validation loss decreased (0.682163 --> 0.654348).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.2019773\n",
            "\tspeed: 0.1667s/iter; left time: 160.8967s\n",
            "\titers: 200, epoch: 3 | loss: 0.1839555\n",
            "\tspeed: 0.0721s/iter; left time: 62.3256s\n",
            "Epoch: 3 cost time: 19.233951807022095\n",
            "Epoch: 3, Steps: 266 | Train Loss: 0.1939725 Vali Loss: 0.6813662 Test Loss: 0.5560160\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.1681474\n",
            "\tspeed: 0.1705s/iter; left time: 119.1925s\n",
            "\titers: 200, epoch: 4 | loss: 0.1603079\n",
            "\tspeed: 0.0743s/iter; left time: 44.5134s\n",
            "Epoch: 4 cost time: 19.82275629043579\n",
            "Epoch: 4, Steps: 266 | Train Loss: 0.1669338 Vali Loss: 0.7344906 Test Loss: 0.6479594\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.1572712\n",
            "\tspeed: 0.1752s/iter; left time: 75.8541s\n",
            "\titers: 200, epoch: 5 | loss: 0.1541290\n",
            "\tspeed: 0.0746s/iter; left time: 24.8455s\n",
            "Epoch: 5 cost time: 19.960747718811035\n",
            "Epoch: 5, Steps: 266 | Train Loss: 0.1543546 Vali Loss: 0.7459015 Test Loss: 0.6300232\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            "---------------------- Testing: Probsparse ----------------------\n",
            "test 2857\n",
            "test shape: (89, 32, 24, 7) (89, 32, 24, 7)\n",
            "test shape: (2848, 24, 7) (2848, 24, 7)\n",
            "mse:0.5606166124343872, mae:0.5444223880767822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the attention type to 'full' for Canonical attention\n",
        "args.attn = 'full'\n",
        "\n",
        "# Train and test the Canonical attention model\n",
        "train_and_test('Canonical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLDKvXAaqBFG",
        "outputId": "1d9153dc-a80f-46fc-8745-12036cf688f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: cuda:0\n",
            "---------------------- Start training: Canonical ---------------------->\n",
            "train 8521\n",
            "val 2857\n",
            "test 2857\n",
            "\titers: 100, epoch: 1 | loss: 0.5083330\n",
            "\tspeed: 0.0666s/iter; left time: 99.7370s\n",
            "\titers: 200, epoch: 1 | loss: 0.2721415\n",
            "\tspeed: 0.0670s/iter; left time: 93.5705s\n",
            "Epoch: 1 cost time: 17.792943477630615\n",
            "Epoch: 1, Steps: 266 | Train Loss: 0.3756041 Vali Loss: 0.6687545 Test Loss: 0.6176901\n",
            "Validation loss decreased (inf --> 0.668754).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.2560747\n",
            "\tspeed: 0.1492s/iter; left time: 183.6377s\n",
            "\titers: 200, epoch: 2 | loss: 0.2679474\n",
            "\tspeed: 0.0671s/iter; left time: 75.9449s\n",
            "Epoch: 2 cost time: 17.842905521392822\n",
            "Epoch: 2, Steps: 266 | Train Loss: 0.2380875 Vali Loss: 0.6515129 Test Loss: 0.5402398\n",
            "Validation loss decreased (0.668754 --> 0.651513).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.1508450\n",
            "\tspeed: 0.1507s/iter; left time: 145.3874s\n",
            "\titers: 200, epoch: 3 | loss: 0.1777155\n",
            "\tspeed: 0.0675s/iter; left time: 58.4018s\n",
            "Epoch: 3 cost time: 17.92558979988098\n",
            "Epoch: 3, Steps: 266 | Train Loss: 0.1784975 Vali Loss: 0.7347042 Test Loss: 0.5688589\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.1913636\n",
            "\tspeed: 0.1505s/iter; left time: 105.2192s\n",
            "\titers: 200, epoch: 4 | loss: 0.1464825\n",
            "\tspeed: 0.0676s/iter; left time: 40.4688s\n",
            "Epoch: 4 cost time: 17.919350385665894\n",
            "Epoch: 4, Steps: 266 | Train Loss: 0.1520783 Vali Loss: 0.7417980 Test Loss: 0.5407069\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.1388994\n",
            "\tspeed: 0.1495s/iter; left time: 64.7479s\n",
            "\titers: 200, epoch: 5 | loss: 0.1121270\n",
            "\tspeed: 0.0674s/iter; left time: 22.4577s\n",
            "Epoch: 5 cost time: 17.888505935668945\n",
            "Epoch: 5, Steps: 266 | Train Loss: 0.1383098 Vali Loss: 0.7644841 Test Loss: 0.5676304\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            "---------------------- Testing: Canonical ----------------------\n",
            "test 2857\n",
            "test shape: (89, 32, 24, 7) (89, 32, 24, 7)\n",
            "test shape: (2848, 24, 7) (2848, 24, 7)\n",
            "mse:0.5402398109436035, mae:0.5386751890182495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probsparse_ground_truth = np.load('./results/Probsparse/true.npy')\n",
        "probsparse_predictions = np.load('./results/Probsparse/pred.npy')\n",
        "\n",
        "# Ensure both arrays have the same shape by slicing to match the dimensions\n",
        "probsparse_ground_truth = probsparse_ground_truth[0, -args.pred_len:]\n",
        "probsparse_predictions = probsparse_predictions[0, -args.pred_len:]\n",
        "\n",
        "# Calculate MSE and MAE for the Probsparse attention model\n",
        "probsparse_mse = mean_squared_error(probsparse_ground_truth, probsparse_predictions)\n",
        "probsparse_mae = mean_absolute_error(probsparse_ground_truth, probsparse_predictions)\n",
        "\n",
        "print(f'MSE for Probsparse Attention Model: {probsparse_mse}')\n",
        "print(f'MAE for Probsparse Attention Model: {probsparse_mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ7IGk-1rKsm",
        "outputId": "a4256c81-eeae-4390-abcc-ccaaa1750477"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for Probsparse Attention Model: 0.507151186466217\n",
            "MAE for Probsparse Attention Model: 0.48420387506484985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "canonical_ground_truth = np.load('./results/Canonical/true.npy')\n",
        "canonical_predictions = np.load('./results/Canonical/pred.npy')\n",
        "\n",
        "# Ensure both arrays have the same shape by slicing to match the dimensions\n",
        "canonical_ground_truth = canonical_ground_truth[0, -args.pred_len:]\n",
        "canonical_predictions = canonical_predictions[0, -args.pred_len:]\n",
        "\n",
        "# Calculate MSE and MAE for the Canonical attention model\n",
        "canonical_mse = mean_squared_error(canonical_ground_truth, canonical_predictions)\n",
        "canonical_mae = mean_absolute_error(canonical_ground_truth, canonical_predictions)\n",
        "\n",
        "print(f'MSE for Canonical Attention Model: {canonical_mse}')\n",
        "print(f'MAE for Canonical Attention Model: {canonical_mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHvmHR7mrVYd",
        "outputId": "3df1aa42-53af-489c-efef-3d1452796ea3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for Canonical Attention Model: 0.5316560864448547\n",
            "MAE for Canonical Attention Model: 0.5037769079208374\n"
          ]
        }
      ]
    }
  ]
}